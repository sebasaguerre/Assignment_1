{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pure Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_class = [1, 0]\n",
    "# support functions \n",
    "def sigmoid(values: list):\n",
    "    return [1/(1 + m.exp(-x)) for x in values] \n",
    "\n",
    "def softmax(values: list):\n",
    "    exp_sum = 0\n",
    "    for value in values:\n",
    "        exp_sum += m.exp(value)\n",
    "    \n",
    "    return [(m.exp(x)/exp_sum) for x in values]\n",
    "\n",
    "def weighted_sum(input: list, weights:list, bias: list):\n",
    "    # initialize empty output list\n",
    "    output = [0] * len(weights[0])\n",
    "\n",
    "    # loop over number of weights per input (number of output)\n",
    "    for i in range(len(weights[0])):\n",
    "        # loop over number of input\n",
    "        for j in range(len(input)):\n",
    "            output[i] += input[j]*weights[j][i]\n",
    "        output[i] += bias[i]\n",
    "    \n",
    "    return output\n",
    "    \n",
    "\n",
    "def predict(smax_output):\n",
    "    return target_class[smax_output.index(max(smax_output))]\n",
    "\n",
    "def forwardpass(input, w1, b1, w2, b2):\n",
    "    # feedforward pass\n",
    "    o1 = weighted_sum(input, w1, b1) # first linear combination\n",
    "    h = sigmoid(o1) # sigmoid activation function\n",
    "    o2 = weighted_sum(h, w2, b2) # second linear combination\n",
    "    y = softmax(o2) # sigmoid activation function\n",
    "    pred = predict(y)\n",
    "\n",
    "    return o1, h, o2, y, pred\n",
    "\n",
    "def elemwise_mult(values1: list, values2: list):\n",
    "    return [x * y for x, y in zip(values1, values2)]\n",
    "\n",
    "def deriv_loss(values: list, target_idx):\n",
    "    return [-1/x if values.index(x) == target_idx else 0 for x in values]\n",
    "\n",
    "def deriv_sigmoid(values: list):\n",
    "    sigmoid_vals = sigmoid(values)\n",
    "    return [x*(1 - x) for x in sigmoid_vals]\n",
    "\n",
    "def deriv_softmax(values: list, target_idx):\n",
    "    \n",
    "    deriv = [0] * len(values)\n",
    "    for idx, value in enumerate(values):\n",
    "        if idx == target_idx:\n",
    "            deriv[idx] = value*(1 - value)\n",
    "        else:\n",
    "            deriv[idx] = (-1)*values[target_idx]*value\n",
    "    \n",
    "    return deriv\n",
    "\n",
    "def get_loss(prediction, target_idx):\n",
    "    return - m.log(prediction[target_idx])\n",
    "\n",
    "def backprop(input, w1, b1, w2, b2, target, o1, h, o2, y):\n",
    "    # calculate the loss\n",
    "    target_idx = target_class.index(target) # get target index value \n",
    "    loss = get_loss(y, target_idx)\n",
    "\n",
    "    # backwards pass to compute derivatives\n",
    "    dL_dy = deriv_loss(y, target_idx) # (2, 0)\n",
    "    dL_do2 = elemwise_mult(dL_dy, deriv_softmax(y, target_idx)) # (2, )\n",
    "    dL_dw2 = [[x*y for x in dL_do2] for y in h] # (3, 2)\n",
    "    dL_db2 = dL_do2\n",
    "    dL_dh = [dL_do2[i]*w2[0][i] for i in range(len(dL_do2))]\n",
    "    for i in range(len(dL_do2)):\n",
    "        dL_dh[i] += dL_do2[i]*w2[1][i]\n",
    "    dL_do1 = [dL_dh[i]* d_sig for d_sig in deriv_sigmoid(o1)]\n",
    "    dL_dw1 = [[dL_do1[i]*input[j] for i in range(len(dL_do1))] for j in range(len(input))]\n",
    "    dL_db1 = dL_do1 \n",
    "    \n",
    "    return dL_dw1, dL_db1, dL_dw2, dL_db2, loss\n",
    "\n",
    "def forward_and_backward(input, w1, b1, w2, b2, target):\n",
    "    # feedforward pass\n",
    "    o1, h, o2, y, pred = forwardpass(input, w1, b1, w2, b2)\n",
    "\n",
    "    # backwards pass and get derivatives\n",
    "    dL_dw1, dL_db1, dL_dw2, dL_db2, loss = backprop(input, w1, b1, w2, b2, target, o1, h, o2, y)\n",
    "    \n",
    "    return dL_dw1, dL_db1, dL_dw2, dL_db2, loss\n",
    "\n",
    "def gradient_desent(w1, b1, w2, b2, dL_dw1, dL_db1, dL_dw2, dL_db2, alpha):\n",
    "    # update weights, biases in first layer \n",
    "    for i in range(len(w1)):\n",
    "        b1[i] = b1[i] - alpha*dL_db1[i]\n",
    "        for idx, w in enumerate(w1[i]):\n",
    "            w1[i][idx] = w - alpha*dL_dw1[i][idx]\n",
    "    \n",
    "\n",
    "    # update weights, biases in second layer \n",
    "    for i in range(len(w2)):\n",
    "        b2[i] = b2[i] - alpha*dL_db2[i]\n",
    "        for idx, w in enumerate(w2[i]):\n",
    "            w2[i][idx] = w - alpha*dL_dw2[i][idx]\n",
    "\n",
    "    return w1, b1, w2, b2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testting BackBropagation for derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dL_dw1 = [[0.0, 0.0, 0.0], [-0.0, -0.0, -0.0]]\n",
      "dL_db1 = [0.0, 0.0, 0.0]\n",
      "dL_dw2 = [[-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116], [-0.44039853898894116, 0.44039853898894116]]\n",
      "dL_db2 = [-0.5, 0.5]\n",
      "Loss = 0.6931471805599453\n"
     ]
    }
   ],
   "source": [
    "# initial set up\n",
    "\n",
    "# for testing \n",
    "input = [1, -1]\n",
    "# weights for layer and bias \n",
    "w1 = [[1.0, 1.0, 1.0], [-1.0, -1.0, -1.0]]\n",
    "b1 = [0.0, 0.0, 0.0]\n",
    "w2 = [[1.0, 1.0], [-1.0, -1.0], [-1.0, -1.0]]\n",
    "b2 = [0.0, 0.0]\n",
    "target_class = [1, 0]\n",
    "\n",
    "# testing \n",
    "target = 1\n",
    "derivs  = forward_and_backward(input, w1, b1, w2, b2, target)\n",
    "text = [\"dL_dw1\", \"dL_db1\", \"dL_dw2\", \"dL_db2\", \"Loss\"]\n",
    "for idx, d in enumerate(derivs):\n",
    "    print(f\"{text[idx]} = {d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- assignment 1 --\n",
    "import numpy as np\n",
    "from urllib import request\n",
    "import gzip\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def load_synth(num_train=60_000, num_val=10_000, seed=0):\n",
    "    \"\"\"\n",
    "    Load some very basic synthetic data that should be easy to classify. Two features, so that we can plot the\n",
    "    decision boundary (which is an ellipse in the feature space).\n",
    "    :param num_train: Number of training instances\n",
    "    :param num_val: Number of test/validation instances\n",
    "    :param num_features: Number of features per instance\n",
    "    :return: Two tuples and an integer: (xtrain, ytrain), (xval, yval), num_cls. The first contains a matrix of training\n",
    "     data with 2 features as a numpy floating point array, and the corresponding classification labels as a numpy\n",
    "     integer array. The second contains the test/validation data in the same format. The last integer contains the\n",
    "     number of classes (this is always 2 for this function).\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    THRESHOLD = 0.6\n",
    "    quad = np.asarray([[1, -0.05], [1, .4]])\n",
    "\n",
    "    ntotal = num_train + num_val\n",
    "\n",
    "    x = np.random.randn(ntotal, 2)\n",
    "\n",
    "    # compute the quadratic form\n",
    "    q = np.einsum('bf, fk, bk -> b', x, quad, x)\n",
    "    y = (q > THRESHOLD).astype(int)\n",
    "\n",
    "    return (x[:num_train, :], y[:num_train]), (x[num_train:, :], y[num_train:]), 2\n",
    "\n",
    "def load_mnist(final=False, flatten=True):\n",
    "    \"\"\"\n",
    "    Load the MNIST data.\n",
    "    :param final: If true, return the canonical test/train split. If false, split some validation data from the training\n",
    "       data and keep the test data hidden.\n",
    "    :param flatten: If true, each instance is flattened into a vector, so that the data is returns as a matrix with 768\n",
    "        columns. If false, the data is returned as a 3-tensor preserving each image as a matrix.\n",
    "    :return: Two tuples and an integer: (xtrain, ytrain), (xval, yval), num_cls. The first contains a matrix of training\n",
    "     data and the corresponding classification labels as a numpy integer array. The second contains the test/validation\n",
    "     data in the same format. The last integer contains the number of classes (this is always 2 for this function).\n",
    "     \"\"\"\n",
    "\n",
    "    if not os.path.isfile('mnist.pkl'):\n",
    "        init()\n",
    "\n",
    "    xtrain, ytrain, xtest, ytest = load()\n",
    "    xtl, xsl = xtrain.shape[0], xtest.shape[0]\n",
    "\n",
    "    if flatten:\n",
    "        xtrain = xtrain.reshape(xtl, -1)\n",
    "        xtest  = xtest.reshape(xsl, -1)\n",
    "\n",
    "    if not final: # return the flattened images\n",
    "        return (xtrain[:-5000], ytrain[:-5000]), (xtrain[-5000:], ytrain[-5000:]), 10\n",
    "\n",
    "    return (xtrain, ytrain), (xtest, ytest), 10\n",
    "\n",
    "# Numpy-only MNIST loader. Courtesy of Hyeonseok Jung\n",
    "# https://github.com/hsjeong5/MNIST-for-Numpy\n",
    "\n",
    "filename = [\n",
    "[\"training_images\",\"train-images-idx3-ubyte.gz\"],\n",
    "[\"test_images\",\"t10k-images-idx3-ubyte.gz\"],\n",
    "[\"training_labels\",\"train-labels-idx1-ubyte.gz\"],\n",
    "[\"test_labels\",\"t10k-labels-idx1-ubyte.gz\"]\n",
    "]\n",
    "\n",
    "def download_mnist():\n",
    "    base_url = \"https://peterbloem.nl/files/mnist/\" # \"http://yann.lecun.com/exdb/mnist/\"\n",
    "    for name in filename:\n",
    "        print(\"Downloading \"+name[1]+\"...\")\n",
    "        request.urlretrieve(base_url+name[1], name[1])\n",
    "    print(\"Download complete.\")\n",
    "\n",
    "def save_mnist():\n",
    "    mnist = {}\n",
    "    for name in filename[:2]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=16).reshape(-1,28*28)\n",
    "    for name in filename[-2:]:\n",
    "        with gzip.open(name[1], 'rb') as f:\n",
    "            mnist[name[0]] = np.frombuffer(f.read(), np.uint8, offset=8)\n",
    "    with open(\"mnist.pkl\", 'wb') as f:\n",
    "        pickle.dump(mnist,f)\n",
    "    print(\"Save complete.\")\n",
    "\n",
    "def init():\n",
    "    download_mnist()\n",
    "    save_mnist()\n",
    "\n",
    "def load():\n",
    "    with open(\"mnist.pkl\",'rb') as f:\n",
    "        mnist = pickle.load(f)\n",
    "    return mnist[\"training_images\"], mnist[\"training_labels\"], mnist[\"test_images\"], mnist[\"test_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.09845252, -0.66347829])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtrain[77]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(xtrain, ytrain), (xval, yval), num_cls = load_synth()\n",
    "\n",
    "# Training and plotting functions \n",
    "def plot_loss(loss):\n",
    "    pass\n",
    "\n",
    "def train(xtrain, ytrain, w1, b1, w2, b2, alpha):\n",
    "    loss_sgd= []\n",
    "    sample_size = xtrain.shape[0]\n",
    "\n",
    "    # start training \n",
    "    for i in range(sample_size):\n",
    "        train_instance = np.random.randint(0, sample_size)\n",
    "        # forward and backwards pass, returning the derivatives \n",
    "        dL_dw1, dL_db1, dL_dw2, dL_db2, loss = forward_and_backward(xtrain[train_instance], w1, b1, w2, b2, ytrain[train_instance])\n",
    "        loss_sgd.append(loss)\n",
    "        # update weights\n",
    "        w1, b1, w2, b2 = gradient_desent(w1, b1, w2, b2, dL_dw1, dL_db1, dL_dw2, dL_db2, alpha)\n",
    "    \n",
    "    return w1, b1, w2, b2, loss_sgd\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization of weights \n",
    "w1 = np.random.normal(loc = 0.0, scale = 1.0, size=(2, 3))\n",
    "b1 = np.zeros(3)\n",
    "w2 = np.random.normal(loc = 0.0, scale = 1.0, size = (3,2 ))\n",
    "b2 = np.zeros(2)\n",
    "\n",
    "# train model\n",
    "w1, b1, w2, b2. loss = train(xtrain, ytrain, w1, b1, w2, b2)\n",
    "\n",
    "# plot loss\n",
    "plot_loss(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DSML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
